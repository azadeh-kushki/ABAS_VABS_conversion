{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20560c81-e801-4200-b289-34c2b2ec236c",
   "metadata": {},
   "source": [
    "**Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ef5abf-d571-4043-929b-eefbe51d2205",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_regressors(X, y):\n",
    "    \n",
    "    \"\"\"\n",
    "    This method take X and y and runs the regressors used for this paper\n",
    "    \"\"\"\n",
    "    # specify number of cross validation foles\n",
    "    num_folds = 10\n",
    "    \n",
    "    # setup data structures to hold the results\n",
    "    lr_results = -1*np.ones(num_folds, dtype=float)\n",
    "    rr_results = -1*np.ones(num_folds, dtype=float)\n",
    "    lasso_results = -1*np.ones(num_folds, dtype=float)\n",
    "    EN_results = -1*np.ones(num_folds, dtype=float)\n",
    "    RF_results = -1*np.ones(num_folds, dtype=float)\n",
    "    GB_results = -1*np.ones(num_folds, dtype=float)\n",
    "    AB_results = -1*np.ones(num_folds, dtype=float)\n",
    "    \n",
    "    # these variables are created for stratified cross validation\n",
    "    y_cat, bins = pd.cut(y, bins = [10, 60, 70, 80, 90, 100, 110, 150], labels = range(7), retbins=True, ordered = True)\n",
    "    y_cat_2 = pd.factorize( y_cat )[0]\n",
    "    \n",
    "    cat = pd.DataFrame({'VABS':y_cat_2, 'Sex':X.sex, 'Dx':X.diagnosis})\n",
    "    cat['combination'] = cat[['Sex', 'Dx','VABS']].agg(tuple, axis=1)\n",
    "    cat['dummy'] = cat['combination'].factorize()[0]\n",
    "\n",
    "    # stratified cross valudation\n",
    "    skf = StratifiedKFold(n_splits=num_folds, shuffle = True, random_state = 20)\n",
    "\n",
    "    fold = 0\n",
    "    \n",
    "    # run the classification models for each fold\n",
    "    for train, test in skf.split(X, cat['dummy']):\n",
    "        \n",
    "        print('*** fold: ', fold, ' ***')\n",
    "        \n",
    "        # get the training and testing sets\n",
    "        X_train = X.iloc[train,:]\n",
    "        X_test = X.iloc[test,:]\n",
    "        y_train = y.iloc[train]\n",
    "        y_test = y.iloc[test]\n",
    "        \n",
    "        # scale the inputs\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(X_train)\n",
    "        X_train_scaled = pd.DataFrame(scaler.transform(X_train))\n",
    "        X_test_scaled = pd.DataFrame(scaler.transform(X_test))\n",
    "    \n",
    "\n",
    "        # Linear regression\n",
    "        lr = LinearRegression().fit(X_train, y_train)\n",
    "        lr_results[fold]=median_absolute_error(y_test,lr.predict(X_test))\n",
    "\n",
    "        # Ridge\n",
    "        clf = RidgeCV(alphas=[0.001,0.01,1,10]).fit(X_train,y_train)\n",
    "        rr = Ridge(alpha=clf.alpha_).fit(X_train, y_train) \n",
    "        rr_results[fold]= median_absolute_error(y_test,rr.predict(X_test))\n",
    "        print('ridge: ',clf.alpha_) \n",
    "\n",
    "        # Lasso\n",
    "        ## parameter selection\n",
    "        clf = LassoCV(alphas=[0.001,0.01,1,10]).fit(X_train, y_train)\n",
    "        model_lasso = Lasso(alpha=clf.alpha_).fit(X_train, y_train) \n",
    "        lasso_results[fold]= median_absolute_error(y_test,model_lasso.predict(X_test))\n",
    "        print('lasso: ',clf.alpha_) \n",
    "\n",
    "        # ElasticNet\n",
    "        ## parameter selection\n",
    "        clf = ElasticNetCV(l1_ratio=[0.01, .1, .5, .95, .99, 1], alphas=[0.01,0.1,0.5,1]).fit(X_train, y_train)\n",
    "        model_EN = ElasticNet(alpha=clf.alpha_, l1_ratio=clf.l1_ratio_).fit(X_train, y_train) \n",
    "        EN_results[fold]= median_absolute_error(y_test,model_EN.predict(X_test))\n",
    "        print('EN: ',clf.alpha_, clf.l1_ratio_) \n",
    "\n",
    "  \n",
    "        # Adaboost\n",
    "        ## parameter selection\n",
    "        param_grid = {\n",
    "            'n_estimators' : np.arange(start=5, stop=80, step=10),\n",
    "            'learning_rate': np.arange(start=0.001, stop=0.01, step=0.001)\n",
    "        }\n",
    "        ab = AdaBoostRegressor(LinearRegression(), loss='square',random_state = 0)\n",
    "        regressor = GridSearchCV(ab, param_grid,  \n",
    "                        cv = KFold(n_splits=5, shuffle = True, random_state = 0))\n",
    "        regressor.fit(X_train, y_train)\n",
    "        regressor.best_estimator_.fit(X_train, y_train) \n",
    "        AB_results[fold]=median_absolute_error(y_test,regressor.best_estimator_.predict(X_test))\n",
    "        print('AB: ',regressor.best_estimator_.n_estimators, regressor.best_estimator_.learning_rate) \n",
    "\n",
    "        # Random forest\n",
    "        ## parameter selection\n",
    "        param_grid = {\n",
    "            'max_features': np.arange(start=1, stop=X.shape[1], step=1),\n",
    "            'n_estimators': np.arange(start=50, stop=100, step=10)\n",
    "        }\n",
    "        rf = RandomForestRegressor(criterion='absolute_error',random_state = 0)\n",
    "        regressor = GridSearchCV(rf, param_grid, \n",
    "                        scoring = 'neg_mean_absolute_error', \n",
    "                        cv = KFold(n_splits=5, shuffle = True, random_state = 0))\n",
    "        regressor.fit(X_train, y_train)\n",
    "        regressor.best_estimator_.fit(X_train, y_train) \n",
    "        RF_results[fold]=median_absolute_error(y_test,regressor.best_estimator_.predict(X_test))\n",
    "        print('RF:', regressor.best_estimator_.n_estimators, regressor.best_estimator_.max_features)\n",
    "\n",
    "        # Gradient boosting\n",
    "        ## parameter selection\n",
    "        param_grid = {\n",
    "            'max_depth': np.arange(start=10, stop=50, step=10),\n",
    "            'n_estimators': np.arange(start=60, stop=100, step=10)\n",
    "        }\n",
    "        gb = GradientBoostingRegressor(learning_rate=0.1, random_state=0, loss='absolute_error')\n",
    "        gbregressor = GridSearchCV(gb, param_grid, scoring = 'neg_mean_absolute_error', \n",
    "                    cv = KFold(n_splits=5, shuffle = True, random_state = 0))\n",
    "        gbregressor.fit(X_train, y_train)\n",
    "        gbregressor.best_estimator_.fit(X_train, y_train) \n",
    "        GB_results[fold]=median_absolute_error(y_test,gbregressor.best_estimator_.predict(X_test))   \n",
    "\n",
    "        fold = fold + 1\n",
    "    \n",
    "    # print the results\n",
    "    print('*****')\n",
    "    print('lr   : ', np.median(lr_results), '+/-', np.percentile(lr_results, 75)-np.percentile(lr_results, 25))\n",
    "    print('rr   : ', np.median(rr_results), '+/-', np.percentile(rr_results, 75)-np.percentile(rr_results, 25))\n",
    "    print('lasso: ', np.median(lasso_results), '+/-', np.percentile(lasso_results, 75)-np.percentile(lasso_results, 25))\n",
    "    print('EN   : ', np.median(EN_results), '+/-', np.percentile(EN_results, 75)-np.percentile(EN_results, 25))\n",
    "    print('RF   : ', np.median(RF_results), '+/-', np.percentile(RF_results, 75)-np.percentile(RF_results, 25))\n",
    "    print('AB   : ', np.median(AB_results), '+/-', np.percentile(AB_results, 75)-np.percentile(AB_results, 25))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
